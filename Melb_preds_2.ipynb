{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOSTON HOUSES PREDICTION WITH SCIKIT (PART 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We continue using the Boston houses dataset, trying to improve the results in the previous code. \n",
    "# This time, instead of dropping all the rows with missing values, we will be imputating the values. Sometimes, it also helps to get \n",
    "# better results, adding a new column and indicating if the value was imputated or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suburb            object\n",
      "Address           object\n",
      "Rooms              int64\n",
      "Type              object\n",
      "Price            float64\n",
      "Method            object\n",
      "SellerG           object\n",
      "Date              object\n",
      "Distance         float64\n",
      "Postcode         float64\n",
      "Bedroom2         float64\n",
      "Bathroom         float64\n",
      "Car              float64\n",
      "Landsize         float64\n",
      "BuildingArea     float64\n",
      "YearBuilt        float64\n",
      "CouncilArea       object\n",
      "Lattitude        float64\n",
      "Longtitude       float64\n",
      "Regionname        object\n",
      "Propertycount    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dir = 'https://raw.githubusercontent.com/AleGL92/Scikit-Learn/main/melb_data.csv'\n",
    "melb_data = pd.read_csv(dir)\n",
    "# print(melb_data.describe())\n",
    "# print(melb_data.head())\n",
    "# When we use describe, object type columns don't appear. We could use them to get information like if a Suburb or a Street is good or bad,\n",
    "# making the predicted price of the property vary. But for simplicity, we're leaving them out this time.\n",
    "print(melb_data.dtypes)\n",
    "\n",
    "y = melb_data.Price\n",
    "melb_predictors = melb_data.drop(['Price'], axis=1)         # This drops the entire column\n",
    "X = melb_predictors.select_dtypes(exclude=['object'])       # include = ['float64'] would have also been an option\n",
    "# print(X.dtypes)\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. HANDLING MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model and the error measurement function.\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def get_mae(train_X, val_X, train_y, val_y):\n",
    "    model = RandomForestRegressor(random_state = 0)          # n_estimators means the number of trees for the forest\n",
    "    model.fit(train_X, train_y)\n",
    "    preds = model.predict(val_X)\n",
    "    return mean_absolute_error(val_y, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Dropping rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping rows we get: 182586.1992983871\n"
     ]
    }
   ],
   "source": [
    "melb_data2 = melb_data.dropna(axis = 0)\n",
    "y2 = melb_data2.Price\n",
    "\n",
    "melb_predictors2 = melb_data2.drop(['Price'], axis=1)         # This drops the entire column\n",
    "X2 = melb_predictors2.select_dtypes(exclude=['object']) \n",
    "\n",
    "train_X2, val_X2, train_y2, val_y2 = train_test_split(X2, y2, train_size = 0.8, test_size = 0.2, random_state = 0)\n",
    "print('Dropping rows we get: {}'.format(get_mae(train_X2, val_X2, train_y2, val_y2)))\n",
    "\n",
    "# Our best result (in the previous code) was 173864 so far. This time, we didn't do that well and we got 182586.\n",
    "# It also looks like the selected cols previously: \n",
    "# m_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "# give a better result than just excluding those cols that aren't objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Dropping columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping columns we get: 175703.48185157913\n"
     ]
    }
   ],
   "source": [
    "cols_missing_vals = [col for col in train_X.columns if train_X[col].isnull().any()]\n",
    "# print(cols_missing_vals)\n",
    "\n",
    "rtrain_X = train_X.drop(cols_missing_vals, axis = 1)\n",
    "rval_X = val_X.drop(cols_missing_vals, axis = 1)\n",
    "print('Dropping columns we get: {}'.format(get_mae(rtrain_X, rval_X, train_y, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Using simple imputation of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a simple imputer we get: 169237.0268668034\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer()\n",
    "itrain_X = pd.DataFrame(imputer.fit_transform(train_X))\n",
    "ival_X = pd.DataFrame(imputer.transform(val_X))\n",
    "\n",
    "# We have to name back the columns. The imputer removes them.\n",
    "itrain_X.columns = train_X.columns\n",
    "ival_X.columns = val_X.columns\n",
    "\n",
    "print('Using a simple imputer we get: {}'.format(get_mae(itrain_X, ival_X, train_y, val_y)))\n",
    "# We got the best results so far (169237)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 Using extended imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using an extended imputer we get: 169795.45249719475\n"
     ]
    }
   ],
   "source": [
    "train_X_ext = train_X.copy()\n",
    "val_X_ext = val_X.copy()\n",
    "\n",
    "for col in cols_missing_vals:\n",
    "    train_X_ext[col + '_was_missing'] = train_X_ext[col].isnull()       #Reminder: Isnull() returns 1 if null, 0 if not\n",
    "    val_X_ext[col + '_was_missing'] = val_X_ext[col].isnull()\n",
    "\n",
    "imputer = SimpleImputer()\n",
    "itrain_X_ext = pd.DataFrame(imputer.fit_transform(train_X_ext))\n",
    "ival_X_ext = pd.DataFrame(imputer.transform(val_X_ext))\n",
    "\n",
    "# We have to name back the columns. The imputer removes them.\n",
    "itrain_X_ext.columns = train_X_ext.columns\n",
    "ival_X_ext.columns = val_X_ext.columns\n",
    "\n",
    "print('Using an extended imputer we get: {}'.format(get_mae(itrain_X_ext, ival_X_ext, train_y, val_y)))\n",
    "\n",
    "# The results were good but just slightly better than the simple imputer (169795). Making new columns to get almost the same result is not \n",
    "# worth it. Maybe in a different dataset this could perform better, so it's still a valid method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10864, 12)\n",
      "Car               49\n",
      "BuildingArea    5156\n",
      "YearBuilt       4307\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# It's useful to check how many missing values there are:\n",
    "# Shape of training data (num_rows, num_columns)\n",
    "print(train_X.shape)\n",
    "# Number of missing values in each column of training data\n",
    "missing_val_count_by_column = (train_X.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])\n",
    "# We can see that the columns Car, BuildingArea and Yearbuilt, where the ones with missing values. The last 2 with 5156 and 4307 missing\n",
    "# values respectively. Car has just 49 missing. But in any those, dropping the entire columns means losing more than half of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. CATEGORICAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suburb            object\n",
      "Address           object\n",
      "Rooms              int64\n",
      "Type              object\n",
      "Price            float64\n",
      "Method            object\n",
      "SellerG           object\n",
      "Date              object\n",
      "Distance         float64\n",
      "Postcode         float64\n",
      "Bedroom2         float64\n",
      "Bathroom         float64\n",
      "Car              float64\n",
      "Landsize         float64\n",
      "BuildingArea     float64\n",
      "YearBuilt        float64\n",
      "CouncilArea       object\n",
      "Lattitude        float64\n",
      "Longtitude       float64\n",
      "Regionname        object\n",
      "Propertycount    float64\n",
      "dtype: object\n",
      "Columns with missing values: ['Car', 'BuildingArea', 'YearBuilt', 'CouncilArea'] (This are excluded)\n",
      "Columns with low cardinality:  ['Type', 'Method', 'Regionname']\n",
      "Columns with numerical values:  ['Rooms', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude', 'Propertycount']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Method</th>\n",
       "      <th>Regionname</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Bedroom2</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longtitude</th>\n",
       "      <th>Propertycount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Southern Metropolitan</td>\n",
       "      <td>2</td>\n",
       "      <td>12.2</td>\n",
       "      <td>3147.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>586.0</td>\n",
       "      <td>-37.8683</td>\n",
       "      <td>145.1082</td>\n",
       "      <td>2894.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7628</th>\n",
       "      <td>t</td>\n",
       "      <td>S</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3057.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3886.0</td>\n",
       "      <td>-37.7762</td>\n",
       "      <td>144.9771</td>\n",
       "      <td>5533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>h</td>\n",
       "      <td>SP</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3067.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>-37.8093</td>\n",
       "      <td>144.9944</td>\n",
       "      <td>4019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6889</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>12.4</td>\n",
       "      <td>3060.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>684.0</td>\n",
       "      <td>-37.7174</td>\n",
       "      <td>144.9664</td>\n",
       "      <td>5070.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Eastern Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>13.9</td>\n",
       "      <td>3108.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>811.0</td>\n",
       "      <td>-37.7927</td>\n",
       "      <td>145.1386</td>\n",
       "      <td>9028.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Type Method             Regionname  Rooms  Distance  Postcode  Bedroom2  \\\n",
       "433     h      S  Southern Metropolitan      2      12.2    3147.0       2.0   \n",
       "7628    t      S  Northern Metropolitan      2       4.5    3057.0       2.0   \n",
       "2       h     SP  Northern Metropolitan      3       2.5    3067.0       3.0   \n",
       "6889    h      S  Northern Metropolitan      3      12.4    3060.0       3.0   \n",
       "2169    h      S   Eastern Metropolitan      3      13.9    3108.0       3.0   \n",
       "\n",
       "      Bathroom  Landsize  Lattitude  Longtitude  Propertycount  \n",
       "433        1.0     586.0   -37.8683    145.1082         2894.0  \n",
       "7628       1.0    3886.0   -37.7762    144.9771         5533.0  \n",
       "2          2.0     134.0   -37.8093    144.9944         4019.0  \n",
       "6889       1.0     684.0   -37.7174    144.9664         5070.0  \n",
       "2169       1.0     811.0   -37.7927    145.1386         9028.0  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we will see different aproaches to handle categorical variables, such as the ones in the 'object' columns.\n",
    "# melb_predictors.head()\n",
    "print(melb_data.dtypes)\n",
    "# We can see there are some columns with type object, that were excluded in the predictions before. Now we'll keep some of them.\n",
    "\n",
    "y3 = melb_data.Price\n",
    "X3 = melb_data.drop(['Price'], axis=1)\n",
    "X3_train_full, X3_valid_full, y3_train, y3_valid = train_test_split(X3, y3, train_size=0.8, test_size=0.2)\n",
    "\n",
    "# For simplicity, we'll continue dropping columns with missing values. We could also apply on of the imputations methods above.\n",
    "# ['Car', 'BuildingArea', 'YearBuilt', 'CouncilArea']\n",
    "cw_missing = [col for col in X3_train_full.columns if X3_train_full[col].isnull().any()]\n",
    "print(f'Columns with missing values: {cw_missing} (This are excluded)' )\n",
    "X3_train_full_d = X3_train_full.drop(cw_missing, axis = 1)      # ,inplace = True to modify directly in the original DF.\n",
    "X3_valid_full_d = X3_valid_full.drop(cw_missing, axis = 1)\n",
    "\n",
    "# Also for simplicity, we'll just convert to categorical values, those columns having less than 10 unique values. This is called \n",
    "# low cardinality. ['Type', 'Method', 'Regionname']\n",
    "lc_cols = [col for col in X3_train_full_d.columns if (X3_train_full_d[col].nunique() < 10 and X3_train_full_d[col].dtype == 'object')]\n",
    "print('Columns with low cardinality: ', lc_cols)\n",
    "\n",
    "# Now we select numerical columns\n",
    "n_cols = [col for col in X3_train_full_d.columns if X3_train_full_d[col].dtype in ['int64', 'float']]\n",
    "print('Columns with numerical values: ', n_cols)\n",
    "\n",
    "# Now we have all the columns that will be used; those that are numerical and those that are categorical, but dont have more than 10\n",
    "# different categorical values.\n",
    "my_cols = lc_cols + n_cols\n",
    "X_train_f = X3_train_full_d[my_cols].copy()\n",
    "X_val_f = X3_valid_full_d[my_cols].copy()\n",
    "X_train_f.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the model again and the measure error function (MAE)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def get_mae(X_train, X_val, y_train, y_val):\n",
    "    model = RandomForestRegressor(n_estimators = 100, random_state = 1)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, preds)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Drop categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE dropping categorical values:  177651.28653005994\n"
     ]
    }
   ],
   "source": [
    "# We'll actually use this just as a reference to compare with the folowing methods. We are dropping columns here.\n",
    "dX_train_f = X_train_f.select_dtypes(exclude = ['object'])\n",
    "dX_val_f = X_val_f.select_dtypes(exclude = ['object'])\n",
    "\n",
    "# print(dX_train_f.describe())\n",
    "# print(dX_val_f.describe())\n",
    "# print(y3_train.describe())\n",
    "# print(y3_valid.describe())\n",
    "\n",
    "print('MAE dropping categorical values: ', get_mae(dX_train_f, dX_val_f, y3_train, y3_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE using ordinal encoding:  167099.28154618497\n"
     ]
    }
   ],
   "source": [
    "# This time we'll use an ordinal encoder, which asigns a number to each categorical value, so that they can be classified.\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "# It's useful to make a copy of the original DS, to avoind changing it during the encoding\n",
    "eX_train = X_train_f.copy()\n",
    "eX_val = X_val_f.copy()\n",
    "\n",
    "# Apply ordinal encoder to each column with categorical data, but just to those selected before, with low cardinality.\n",
    "encoder = OrdinalEncoder()\n",
    "eX_train[lc_cols] = encoder.fit_transform(eX_train[lc_cols])\n",
    "eX_val[lc_cols] = encoder.fit_transform(eX_val[lc_cols])\n",
    "\n",
    "print('MAE using ordinal encoding: ', get_mae(eX_train, eX_val, y3_train, y3_valid))\n",
    "# Using the ordinal encoder, we got 176296, which is better than just dropping the categorical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE using one-hot enconding:  167627.49990247274\n"
     ]
    }
   ],
   "source": [
    "# This time we'll add columns representing each categorical value. The row value will be 1 if it's the right categorical value or \n",
    "# 0 if it's not. This way we can classify all the categorical values.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oh_e = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "# handle_unknown; When this parameter is set to ‘ignore’ and an unknown category is encountered during transform, the resulting one-hot encoded \n",
    "# columns for this feature will be all zeros.\n",
    "# sparse; will return sparse matrix if set True else will return an array.\n",
    "\n",
    "# This time, we'll just encode the categorical columns, because the OneHot encoded columns will be returned in addition.\n",
    "# Then we remove from the DS the categorical columns and, finally, we add the result columns from the encoding.\n",
    "\n",
    "oh_cols_train = pd.DataFrame(oh_e.fit_transform(X_train_f[lc_cols]))\n",
    "oh_cols_val = pd.DataFrame(oh_e.fit_transform(X_val_f[lc_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "oh_cols_train.index = X_train_f.index\n",
    "oh_cols_val.index = X_val_f.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "pre_X_train = X_train_f.drop(lc_cols, axis = 1)\n",
    "pre_X_val = X_val_f.drop(lc_cols, axis = 1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "ohX_train = pd.concat([pre_X_train, oh_cols_train], axis = 1)\n",
    "ohX_val = pd.concat([pre_X_val, oh_cols_val], axis = 1)\n",
    "\n",
    "print('MAE using one-hot enconding: ', get_mae(ohX_train, ohX_val, y3_train, y3_valid))\n",
    "# The MAE is 173099, which is slightly better than ordinal encoding. In this case I wouldnt choose one-hot encoding, because the results\n",
    "# are very similar, but we're adding more columns, making the DS heavier and slower computing. However, one-hot encoding is supposed to \n",
    "# return better results than ordinal encoding. Not in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. PIPELINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time, we'll be applying some of the previous methods in a few lines. It's much better for clarity and it also helps avoiding mistakes.\n",
    "# The pipeline will have 2 parts, one for numerical columns and another one for categorical columns.\n",
    "# As a remainder, in step 2 we had separated both of them and finally concatenated as my_cols = lc_cols + n_cols\n",
    "# Numerical columns were all that had format int64 or float and for categorical columns we just those with a cardinality lower than 10.\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transform = SimpleImputer(strategy = 'mean')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transform = Pipeline(steps = [\n",
    "    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "    ('num', numerical_transform, n_cols),\n",
    "    ('cat', categorical_transform, lc_cols) \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE using a pipeline: 166988.97910290578\n"
     ]
    }
   ],
   "source": [
    "# Creating the model, the pipeline and, evaluating the pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "model = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
    "my_pipeline = Pipeline(steps = [\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# With the pipeline, we supply the unprocessed features in X_valid to the predict() command, and the pipeline automatically preprocesses \n",
    "# the features before generating predictions. (However, without a pipeline, we have to remember to preprocess the validation data before \n",
    "# making predictions.)\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train_f, y3_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_val_f)\n",
    "\n",
    "# Evaluate the model\n",
    "MAE = mean_absolute_error(y3_valid, preds)\n",
    "print('MAE using a pipeline:', MAE)\n",
    "# We got good results, similar to the best so far, "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63f0348d2a946343dffa286e77d1d2673ee07cffce91d303b3699736551171f0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
