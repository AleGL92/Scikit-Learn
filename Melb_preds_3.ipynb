{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOSTON HOUSES PREDICTION WITH SCIKIT (PART 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We continue using the Boston houses dataset, trying to improve the results in the previous code. \n",
    "# This time we'll see cross validation, XGBoost and some ideas about data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAEs using cross-validation:\n",
      " [301628.7893587  303164.4782723  287298.331666   236061.84754543\n",
      " 260383.45111427]\n"
     ]
    }
   ],
   "source": [
    "# In the previous code, when we used the train_test_split command to get 80% of the data as train data and the resulting 20% as validation data\n",
    "# we don't know what lines of the data we're getting. Maybe they are the first 80%, maybe the last 80% or maybe there is 10% validation, then\n",
    "# 80% training and then, the last 10% for validation. This could affect the error we're getting, so we use cross-validation to check wich way\n",
    "# is better. In big DS it might not be a good idea, as it could be really slow.\n",
    "\n",
    "import pandas as pd\n",
    "dir = 'https://raw.githubusercontent.com/AleGL92/Scikit-Learn/main/melb_data.csv'\n",
    "melb_data = pd.read_csv(dir)\n",
    "cols = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
    "X = melb_data[cols]\n",
    "y = melb_data.Price\n",
    "\n",
    "# Then, we define a pipeline that uses an imputer to fill in missing values and a random forest model to make predictions.\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "my_pipeline = Pipeline(steps = [\n",
    "    ('preprocessor', SimpleImputer()),\n",
    "    ('model', RandomForestRegressor(n_estimators = 50, random_state = 0))\n",
    "])\n",
    "\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "MAE = -1 * cross_val_score(my_pipeline, X, y, cv = 5, scoring = 'neg_mean_absolute_error')\n",
    "print(\"MAEs using cross-validation:\\n\", MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE using XGBoost:  244251.88284931885\n"
     ]
    }
   ],
   "source": [
    "# Gradient boosting is a method that goes through cycles to iteratively add models into an ensemble.\n",
    "# A first prediction is made, from this one a loss function is calculated. This loss function is then used to fit a new model, then this model will be added to the ensemble.\n",
    "# Within a set number of iterations, we can sometines get better predictions than using a Decission Tree or a Random Forest.\n",
    "# We'll be using XGBooster library for this. Before we split in test and validation data.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "my_model = XGBRegressor()\n",
    "my_model.fit(X_train, y_train)\n",
    "\n",
    "preds = my_model.predict(X_val)\n",
    "print(\"MAE using XGBoost: \", mean_absolute_error(preds, y_val))\n",
    "# We got 244251, wich is the best result so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE using XGBoost, after tuning parameters:  240908.63307713548\n"
     ]
    }
   ],
   "source": [
    "# There are some parameters that affect the results when using XGBoost:\n",
    "\n",
    "# n_estimators specifies how many times to go through the modeling cycle described above. It is equal to the number of models that we include in the ensemble.\n",
    "# Values too low for n_estimators cause underfitting, and values too high, cause overfitting.\n",
    "\n",
    "# early_stopping_rounds offers a way to automatically find the ideal value for n_estimators. Early stopping causes the model to stop iterating when the validation score \n",
    "# stops improving. Since random chance sometimes causes a single round where validation scores don't improve, we need to specify a number for how many rounds of straight \n",
    "# deterioration to allow before stopping.\n",
    "\n",
    "# Instead of getting predictions by simply adding up the predictions from each component model, we can multiply the predictions from each model by a small number \n",
    "# (known as the learning rate) before adding them in. This means each tree we add to the ensemble helps us less. So, we can set a higher value for n_estimators without \n",
    "# overfitting. If we use early stopping, the appropriate number of trees will be determined automatically. By default XGBoost sets learning rate of 0,1.\n",
    "\n",
    "# On larger datasets where runtime is a consideration, we could use parallelism to build your models faster. It's common to set the parameter n_jobs equal to the number of \n",
    "# cores on your machine. On smaller datasets, this won't help.\n",
    "\n",
    "my_model = XGBRegressor(n_estimators = 500, learning_rate = 0.05, n_jobs = 4)\n",
    "my_model.fit(X_train, y_train, \n",
    "            early_stopping_rounds = 5, \n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose = False)\n",
    "\n",
    "preds = my_model.predict(X_val)\n",
    "print(\"MAE using XGBoost, after tuning parameters: \", mean_absolute_error(preds, y_val))\n",
    "# We got 240908, wich is even better than the previous result."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe68149fe8549fceca107e62d5023ede2ad6b4760e922d52c26df0ecc2fe7792"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
